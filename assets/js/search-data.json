{
  
    
        "post0": {
            "title": "Hospitalizations by State",
            "content": "# Imports import os import pandas as pd import csv import kaggle # other imports import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.linear_model import ElasticNet from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures from sklearn.metrics import mean_squared_error, mean_absolute_error, classification_report from sklearn.utils.testing import ignore_warnings from sklearn.exceptions import ConvergenceWarning from copy import copy import seaborn as sns from scipy.stats import norm import matplotlib.dates as mdates # import matplotlib.colors as mcolors # import random # import math # import time # from sklearn.linear_model import LinearRegression, BayesianRidge # from sklearn.model_selection import RandomizedSearchCV from sklearn.tree import DecisionTreeRegressor # from sklearn.svm import SVR from datetime import date, datetime from dateutil.parser import parse import us # import operator # plt.style.use(&#39;fivethirtyeight&#39;) import plotly.graph_objects as go from plotly.subplots import make_subplots %matplotlib inline . C: ProgramData Anaconda3 lib site-packages sklearn utils deprecation.py:143: FutureWarning: The sklearn.utils.testing module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API. warnings.warn(message, FutureWarning) . Covid Tracking Dataset (w/ hospitalised data) . Source: https://covidtracking.com/ . Load and Clean the Data . all_cases = pd.read_csv(&#39;https://covidtracking.com/api/v1/states/daily.csv&#39;) # Delete unecessary rows for row in [&#39;negative&#39;, &#39;pending&#39;, &#39;hash&#39;, &#39;negativeIncrease&#39;, &#39;totalTestResults&#39;, &#39;totalTestResultsIncrease&#39;, &#39;dateChecked&#39;, &#39;fips&#39;, &#39;inIcuCumulative&#39;, &#39;onVentilatorCumulative&#39;, &#39;total&#39;, &#39;posNeg&#39;, &#39;deathIncrease&#39;, &#39;hospitalizedIncrease&#39;, &#39;positiveIncrease&#39;]: del all_cases[row] # TODO missing values # Do we get avg or missing values, or predict them? # See https://developerzen.com/data-mining-handling-missing-values-the-database-bd2241882e72 for i, row in all_cases.iterrows(): # Set Dates s = str(row[&#39;date&#39;]) all_cases.at[i, &#39;date&#39;] = date(year=int(s[0:4]), month=int(s[4:6]), day=int(s[6:8])) # Missing death figures means no death reports yet # These are set to 0 for i, row in all_cases.iterrows(): if np.isnan(row[&#39;death&#39;]): all_cases.at[i, &#39;death&#39;] = 0 . Missing values: Retrieving from other datasets or through merging columns (or both) . The following will be done: . Active Cases: Retrieved from JHU dataset and calculating $active = pos-dead-recovered$ | Beds per State: Retrieved from External Datasets | . # TODO Replace active cases with JHU and/or regression model (Selma) all_cases[&#39;active&#39;] = all_cases[&#39;positive&#39;] - all_cases[&#39;recovered&#39;] - all_cases[&#39;death&#39;] # change location of &#39;active&#39; column cols = list(all_cases) cols.insert(3, cols.pop(cols.index(&#39;active&#39;))) all_cases = all_cases.loc[:, cols] . # Load datasets for US population and Hospital beds per 1000 us_population = pd.read_csv(&#39;data/us_population.csv&#39;) hosp_beds = pd.read_csv(&#39;data/hospital_beds.csv&#39;) state_abbrev = pd.read_csv(&#39;data/us_state_names.csv&#39;) # add state abbreviations to us_population and hospital beds dataframe for state in state_abbrev[&#39;State&#39;].tolist(): # store state abbreviation in variable abbrev = state_abbrev.loc[state_abbrev[&#39;State&#39;] == state, &#39;Abbreviation&#39;].tolist()[0] # add abbrev to new column &#39;Abbreviation&#39; in us_population df us_population.loc[us_population[&#39;State&#39;] == state, &#39;Abbreviation&#39;] = abbrev # add abbrev to new column in hosp_beds df hosp_beds.loc[hosp_beds[&#39;Location&#39;] == state, &#39;Abbreviation&#39;] = abbrev # change order of columns of us_population cols = list(us_population) cols.insert(2, cols.pop(cols.index(&#39;Abbreviation&#39;))) us_population = us_population.loc[:, cols] # drop unnecessary columns of us_population us_population = us_population.drop(columns=[&#39;rank&#39;, &#39;Growth&#39;, &#39;Pop2018&#39;, &#39;Pop2010&#39;, &#39;growthSince2010&#39;, &#39;Percent&#39;, &#39;density&#39;]) # drop unnecessary columns of hosp_beds hosp_beds = hosp_beds.drop(columns=[&#39;Location&#39;, &#39;State/Local Government&#39;, &#39;Non-Profit&#39;, &#39;For-Profit&#39;]) # change order of columns of hosp_beds cols = list(hosp_beds) cols.insert(0, cols.pop(cols.index(&#39;Abbreviation&#39;))) hosp_beds = hosp_beds.loc[:, cols] . us_population.head() . State Abbreviation Pop . 0 Alabama | AL | 4908621 | . 1 Alaska | AK | 734002 | . 2 Arizona | AZ | 7378494 | . 3 Arkansas | AR | 3038999 | . 4 California | CA | 39937489 | . hosp_beds.head() . Abbreviation Total . 0 NaN | 2.4 | . 1 AL | 3.1 | . 2 AK | 2.2 | . 3 AZ | 1.9 | . 4 AR | 3.2 | . # filter out non-existing states like &#39;AS&#39; all_cases = all_cases[all_cases[&#39;state&#39;].isin(state_abbrev[&#39;Abbreviation&#39;].tolist())] . # see what filtered dataframe looks like all_cases.head() . date state positive active hospitalizedCurrently hospitalizedCumulative inIcuCurrently onVentilatorCurrently recovered dataQualityGrade ... totalTestsViral positiveTestsViral negativeTestsViral positiveCasesViral commercialScore negativeRegularScore negativeScore positiveScore score grade . 0 2020-07-03 | AK | 1063.0 | 509.0 | 25.0 | NaN | NaN | 3.0 | 539.0 | A | ... | 120208.0 | NaN | NaN | NaN | 0 | 0 | 0 | 0 | 0 | NaN | . 1 2020-07-03 | AL | 41865.0 | 18777.0 | 812.0 | 2883.0 | NaN | NaN | 22082.0 | B | ... | NaN | NaN | NaN | 41362.0 | 0 | 0 | 0 | 0 | 0 | NaN | . 2 2020-07-03 | AR | 22622.0 | 6177.0 | 285.0 | 1517.0 | NaN | 70.0 | 16164.0 | A | ... | NaN | NaN | NaN | 22622.0 | 0 | 0 | 0 | 0 | 0 | NaN | . 4 2020-07-03 | AZ | 91858.0 | 79592.0 | 3013.0 | 5018.0 | 741.0 | 489.0 | 10478.0 | A+ | ... | 577919.0 | NaN | NaN | 91396.0 | 0 | 0 | 0 | 0 | 0 | NaN | . 5 2020-07-03 | CA | 248235.0 | NaN | 7024.0 | NaN | 1871.0 | NaN | NaN | B | ... | 4448176.0 | NaN | NaN | 248235.0 | 0 | 0 | 0 | 0 | 0 | NaN | . 5 rows × 25 columns . # Split dataframes by date df_split_by_date = dict(tuple(all_cases.groupby(&#39;date&#39;))) # Split dataframes by state df_split_by_state = dict(tuple(all_cases.groupby(&#39;state&#39;))) . # merge dataframes us_population and all_cases df_merge_uspop = all_cases.merge(us_population, how=&#39;left&#39;, left_on=&#39;state&#39;, right_on=&#39;Abbreviation&#39;) df_merge_uspop = df_merge_uspop.drop(columns=[&#39;Abbreviation&#39;]) df_merge_uspop = df_merge_uspop.rename(columns={&#39;Pop&#39;: &#39;population&#39;}) # change location of &#39;population&#39; column cols = list(df_merge_uspop) cols.insert(2, cols.pop(cols.index(&#39;population&#39;))) df_merge_uspop = df_merge_uspop.loc[:, cols] # merge dataframes hosp_beds and df_merge_uspop df_merge_hosp = df_merge_uspop.merge(hosp_beds, how=&#39;left&#39;, left_on=&#39;state&#39;, right_on=&#39;Abbreviation&#39;) df_merge_hosp = df_merge_hosp.drop(columns=[&#39;Abbreviation&#39;]) all_cases = df_merge_hosp.rename(columns={&#39;Total&#39;: &#39;bedsPerThousand&#39;}) . all_cases.head() . date state population positive active hospitalizedCurrently hospitalizedCumulative inIcuCurrently onVentilatorCurrently recovered ... negativeTestsViral positiveCasesViral commercialScore negativeRegularScore negativeScore positiveScore score grade State bedsPerThousand . 0 2020-07-03 | AK | 734002 | 1063.0 | 509.0 | 25.0 | NaN | NaN | 3.0 | 539.0 | ... | NaN | NaN | 0 | 0 | 0 | 0 | 0 | NaN | Alaska | 2.2 | . 1 2020-07-03 | AL | 4908621 | 41865.0 | 18777.0 | 812.0 | 2883.0 | NaN | NaN | 22082.0 | ... | NaN | 41362.0 | 0 | 0 | 0 | 0 | 0 | NaN | Alabama | 3.1 | . 2 2020-07-03 | AR | 3038999 | 22622.0 | 6177.0 | 285.0 | 1517.0 | NaN | 70.0 | 16164.0 | ... | NaN | 22622.0 | 0 | 0 | 0 | 0 | 0 | NaN | Arkansas | 3.2 | . 3 2020-07-03 | AZ | 7378494 | 91858.0 | 79592.0 | 3013.0 | 5018.0 | 741.0 | 489.0 | 10478.0 | ... | NaN | 91396.0 | 0 | 0 | 0 | 0 | 0 | NaN | Arizona | 1.9 | . 4 2020-07-03 | CA | 39937489 | 248235.0 | NaN | 7024.0 | NaN | 1871.0 | NaN | NaN | ... | NaN | 248235.0 | 0 | 0 | 0 | 0 | 0 | NaN | California | 1.8 | . 5 rows × 28 columns . # Calculate the total beds, and add the column all_cases[&#39;total_beds&#39;] = all_cases[&#39;population&#39;] / 1000 * all_cases[&#39;bedsPerThousand&#39;] . # change abbreviations to state names all_cases = all_cases.rename(columns={&#39;state&#39;: &#39;abbrev&#39;}) all_cases = all_cases.rename(columns={&#39;State&#39;: &#39;state&#39;}) . # change location of &#39;state&#39; column cols = list(all_cases) cols.insert(1, cols.pop(cols.index(&#39;state&#39;))) all_cases = all_cases.loc[:, cols] . all_cases.head() . date state abbrev population positive active hospitalizedCurrently hospitalizedCumulative inIcuCurrently onVentilatorCurrently ... negativeTestsViral positiveCasesViral commercialScore negativeRegularScore negativeScore positiveScore score grade bedsPerThousand total_beds . 0 2020-07-03 | Alaska | AK | 734002 | 1063.0 | 509.0 | 25.0 | NaN | NaN | 3.0 | ... | NaN | NaN | 0 | 0 | 0 | 0 | 0 | NaN | 2.2 | 1614.8044 | . 1 2020-07-03 | Alabama | AL | 4908621 | 41865.0 | 18777.0 | 812.0 | 2883.0 | NaN | NaN | ... | NaN | 41362.0 | 0 | 0 | 0 | 0 | 0 | NaN | 3.1 | 15216.7251 | . 2 2020-07-03 | Arkansas | AR | 3038999 | 22622.0 | 6177.0 | 285.0 | 1517.0 | NaN | 70.0 | ... | NaN | 22622.0 | 0 | 0 | 0 | 0 | 0 | NaN | 3.2 | 9724.7968 | . 3 2020-07-03 | Arizona | AZ | 7378494 | 91858.0 | 79592.0 | 3013.0 | 5018.0 | 741.0 | 489.0 | ... | NaN | 91396.0 | 0 | 0 | 0 | 0 | 0 | NaN | 1.9 | 14019.1386 | . 4 2020-07-03 | California | CA | 39937489 | 248235.0 | NaN | 7024.0 | NaN | 1871.0 | NaN | ... | NaN | 248235.0 | 0 | 0 | 0 | 0 | 0 | NaN | 1.8 | 71887.4802 | . 5 rows × 29 columns . Load and clean JHU data | Merge JHU dataset with main dataset | . # This cell takes some time, as it needs to connect to Kaggle Servers to retrieve data kaggle.api.authenticate() kaggle.api.dataset_download_files(&#39;benhamner/jhucovid19&#39;, path=&#39;./kaggle/input/jhucovid19/&#39;, unzip=True) . # Get Time-Series Data of cases as Pandas DataFrame dir_jhu = &#39;./kaggle/input/jhucovid19/csse_covid_19_data/csse_covid_19_daily_reports&#39; df_list = [] for dirname, _, files in os.walk(dir_jhu): for file in files: if &#39;gitignore&#39; not in file and &#39;README&#39; not in file: full_dir = os.path.join(dirname, file) df_list.append(pd.read_csv(full_dir)) jhu_df = pd.concat(df_list, axis=0, ignore_index=True, sort=True) # convert Last Update columns to datetime format jhu_df.loc[:, &#39;Last Update&#39;] = pd.to_datetime(jhu_df[&#39;Last Update&#39;]).apply(lambda x: x.date()) jhu_df.loc[:, &#39;Last_Update&#39;] = pd.to_datetime(jhu_df[&#39;Last_Update&#39;]).apply(lambda x: x.date()) # Combine Last Update with Last_Update jhu_df[&#39;LastUpdate&#39;] = jhu_df[&#39;Last_Update&#39;].combine_first(jhu_df[&#39;Last Update&#39;]) # Combine Country/Region with Country_Region jhu_df[&#39;CountryRegion&#39;] = jhu_df[&#39;Country/Region&#39;].combine_first(jhu_df[&#39;Country_Region&#39;]) # Retrieve only US data jhu_df = jhu_df[jhu_df[&#39;CountryRegion&#39;]==&#39;US&#39;] # Combine Province/State with Province_State jhu_df[&#39;ProvinceState&#39;] = jhu_df[&#39;Province/State&#39;].combine_first(jhu_df[&#39;Province_State&#39;]) # Drop unnecessary columns jhu_df = jhu_df.drop([&#39;Admin2&#39;, &#39;Lat&#39;, &#39;Latitude&#39;, &#39;Long_&#39;, &#39;Longitude&#39;, &#39;Combined_Key&#39;, &#39;Country/Region&#39;, &#39;Country_Region&#39;, &#39;Province/State&#39;, &#39;Province_State&#39;, &#39;Last Update&#39;, &#39;Last_Update&#39;, &#39;FIPS&#39;], axis=1) # Change column order cols = list(jhu_df) cols.insert(0, cols.pop(cols.index(&#39;CountryRegion&#39;))) cols.insert(1, cols.pop(cols.index(&#39;ProvinceState&#39;))) cols.insert(2, cols.pop(cols.index(&#39;LastUpdate&#39;))) jhu_df = jhu_df.loc[:, cols] # Change region to known US states state_abbrs_dict = {} for state in us.states.STATES: state_abbrs_dict[state.abbr] = state.name def toState(input_state, mapping): abbreviation = input_state.rstrip()[-2:] try: return_value = mapping[abbreviation] except KeyError: return_value = input_state return return_value jhu_df[&#39;ProvinceState&#39;] = jhu_df[&#39;ProvinceState&#39;].apply(lambda x: toState(x, state_abbrs_dict) if x != &#39;Washington, D.C.&#39; else &#39;District of Columbia&#39;) # Filter out unknown states jhu_df = jhu_df[jhu_df[&#39;ProvinceState&#39;].isin(all_cases.state.unique().tolist())] # Merge-sum rows with same date and State jhu_df = jhu_df.groupby([&#39;LastUpdate&#39;, &#39;ProvinceState&#39;]).agg( { &#39;Active&#39;: sum, &#39;Confirmed&#39;: sum, &#39;Deaths&#39;: sum, &#39;Recovered&#39;: sum } ).reset_index() jhu_df.tail() . LastUpdate ProvinceState Active Confirmed Deaths Recovered . 5802 2020-07-01 | Virginia | 61024.0 | 62787.0 | 1763.0 | 0.0 | . 5803 2020-07-01 | Washington | 31492.0 | 32824.0 | 1332.0 | 0.0 | . 5804 2020-07-01 | West Virginia | 2812.0 | 2905.0 | 93.0 | 0.0 | . 5805 2020-07-01 | Wisconsin | 27875.0 | 28659.0 | 784.0 | 0.0 | . 5806 2020-07-01 | Wyoming | 1467.0 | 1487.0 | 20.0 | 0.0 | . # Now that we have the JHU dataset relatively cleaned # we can go ahead and merge its data with our main dataset for i, row in all_cases.iterrows(): last_update = all_cases.at[i, &#39;date&#39;] state = all_cases.at[i, &#39;state&#39;] matching_row = jhu_df[jhu_df[&#39;ProvinceState&#39;] == state] matching_row = matching_row[matching_row[&#39;LastUpdate&#39;] == last_update].reset_index() if len(matching_row.values) &gt; 0: #all_cases.at[i, &#39;positive&#39;] = matching_row[&#39;Confirmed&#39;].values[0] all_cases.at[i, &#39;active&#39;] = matching_row[&#39;Active&#39;].values[0] #all_cases.at[i, &#39;recovered&#39;] = matching_row[&#39;Recovered&#39;].values[0] JHU was inconsistent, therefore removed #all_cases.at[i, &#39;death&#39;] = matching_row[&#39;Deaths&#39;].values[0] # Replace unknown recovery numbers with 0 if np.isnan(row[&#39;recovered&#39;]): all_cases.at[i, &#39;recovered&#39;] = 0 if all_cases.at[i, &#39;active&#39;] == 0 or np.isnan(row[&#39;active&#39;]): positive = all_cases.at[i, &#39;positive&#39;] recovered = all_cases.at[i, &#39;recovered&#39;] dead = all_cases.at[i, &#39;death&#39;] all_cases.at[i, &#39;active&#39;] = positive - recovered - dead all_cases.tail() . date state abbrev population positive active hospitalizedCurrently hospitalizedCumulative inIcuCurrently onVentilatorCurrently ... negativeTestsViral positiveCasesViral commercialScore negativeRegularScore negativeScore positiveScore score grade bedsPerThousand total_beds . 6182 2020-01-26 | Washington | WA | 7797095 | 2.0 | 2.0 | NaN | NaN | NaN | NaN | ... | NaN | NaN | 0 | 0 | 0 | 0 | 0 | NaN | 1.7 | 13255.0615 | . 6183 2020-01-25 | Washington | WA | 7797095 | 2.0 | 2.0 | NaN | NaN | NaN | NaN | ... | NaN | NaN | 0 | 0 | 0 | 0 | 0 | NaN | 1.7 | 13255.0615 | . 6184 2020-01-24 | Washington | WA | 7797095 | 2.0 | 2.0 | NaN | NaN | NaN | NaN | ... | NaN | NaN | 0 | 0 | 0 | 0 | 0 | NaN | 1.7 | 13255.0615 | . 6185 2020-01-23 | Washington | WA | 7797095 | 2.0 | 2.0 | NaN | NaN | NaN | NaN | ... | NaN | NaN | 0 | 0 | 0 | 0 | 0 | NaN | 1.7 | 13255.0615 | . 6186 2020-01-22 | Washington | WA | 7797095 | 2.0 | 2.0 | NaN | NaN | NaN | NaN | ... | NaN | NaN | 0 | 0 | 0 | 0 | 0 | NaN | 1.7 | 13255.0615 | . 5 rows × 29 columns . # Save formatted dataset offline in case of disaster dataset_file = &#39;results/all_cases.csv&#39; all_cases.to_csv(dataset_file) . # convert date to datetime format all_cases[&#39;date&#39;] = pd.to_datetime(all_cases[&#39;date&#39;]) . An Exploratory data analysis of the US dataset . Basic triad of the dataset: validating data types and data integrity of each row . dataset_file = &#39;results/all_cases.csv&#39; covid_df = pd.read_csv(dataset_file, index_col=0) # convert date to datetime format covid_df[&#39;date&#39;] = pd.to_datetime(covid_df[&#39;date&#39;]) covid_df.info() # set float format to 3 decimals pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.3f&#39; % x) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 6187 entries, 0 to 6186 Data columns (total 29 columns): date 6187 non-null datetime64[ns] state 6187 non-null object abbrev 6187 non-null object population 6187 non-null int64 positive 6187 non-null float64 active 6187 non-null float64 hospitalizedCurrently 4057 non-null float64 hospitalizedCumulative 3414 non-null float64 inIcuCurrently 2052 non-null float64 onVentilatorCurrently 1789 non-null float64 recovered 6187 non-null float64 dataQualityGrade 5253 non-null object lastUpdateEt 5832 non-null object dateModified 5832 non-null object checkTimeEt 5832 non-null object death 6187 non-null float64 hospitalized 3414 non-null float64 totalTestsViral 1712 non-null float64 positiveTestsViral 585 non-null float64 negativeTestsViral 590 non-null float64 positiveCasesViral 3347 non-null float64 commercialScore 6187 non-null int64 negativeRegularScore 6187 non-null int64 negativeScore 6187 non-null int64 positiveScore 6187 non-null int64 score 6187 non-null int64 grade 0 non-null float64 bedsPerThousand 6187 non-null float64 total_beds 6187 non-null float64 dtypes: datetime64[ns](1), float64(16), int64(6), object(6) memory usage: 1.4+ MB . covid_df.head() . date state abbrev population positive active hospitalizedCurrently hospitalizedCumulative inIcuCurrently onVentilatorCurrently ... negativeTestsViral positiveCasesViral commercialScore negativeRegularScore negativeScore positiveScore score grade bedsPerThousand total_beds . 0 2020-07-03 | Alaska | AK | 734002 | 1063.000 | 509.000 | 25.000 | nan | nan | 3.000 | ... | nan | nan | 0 | 0 | 0 | 0 | 0 | nan | 2.200 | 1614.804 | . 1 2020-07-03 | Alabama | AL | 4908621 | 41865.000 | 18777.000 | 812.000 | 2883.000 | nan | nan | ... | nan | 41362.000 | 0 | 0 | 0 | 0 | 0 | nan | 3.100 | 15216.725 | . 2 2020-07-03 | Arkansas | AR | 3038999 | 22622.000 | 6177.000 | 285.000 | 1517.000 | nan | 70.000 | ... | nan | 22622.000 | 0 | 0 | 0 | 0 | 0 | nan | 3.200 | 9724.797 | . 3 2020-07-03 | Arizona | AZ | 7378494 | 91858.000 | 79592.000 | 3013.000 | 5018.000 | 741.000 | 489.000 | ... | nan | 91396.000 | 0 | 0 | 0 | 0 | 0 | nan | 1.900 | 14019.139 | . 4 2020-07-03 | California | CA | 39937489 | 248235.000 | 241972.000 | 7024.000 | nan | 1871.000 | nan | ... | nan | 248235.000 | 0 | 0 | 0 | 0 | 0 | nan | 1.800 | 71887.480 | . 5 rows × 29 columns . The NaN values may indicate that there were no to few Covid-19 patients at these date points. We further analyse the statistical values of the dataset columns to ensure data integrity and accuracy. . covid_df.describe() # TODO rounding up the numbers . population positive active hospitalizedCurrently hospitalizedCumulative inIcuCurrently onVentilatorCurrently recovered death hospitalized ... negativeTestsViral positiveCasesViral commercialScore negativeRegularScore negativeScore positiveScore score grade bedsPerThousand total_beds . count 6187.000 | 6187.000 | 6187.000 | 4057.000 | 3414.000 | 2052.000 | 1789.000 | 6187.000 | 6187.000 | 3414.000 | ... | 590.000 | 3347.000 | 6187.000 | 6187.000 | 6187.000 | 6187.000 | 6187.000 | 0.000 | 6187.000 | 6187.000 | . mean 6541047.150 | 22448.958 | 20684.635 | 976.125 | 4496.786 | 434.262 | 216.229 | 4891.973 | 1152.022 | 4496.786 | ... | 319529.515 | 33575.520 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 2.626 | 15803.983 | . std 7386652.501 | 48404.581 | 44568.693 | 1864.136 | 13066.426 | 700.329 | 320.857 | 11736.161 | 2995.990 | 13066.426 | ... | 420218.699 | 57939.209 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 0.744 | 16159.025 | . min 567025.000 | 0.000 | 0.000 | 1.000 | 0.000 | 1.000 | 0.000 | 0.000 | 0.000 | 0.000 | ... | 17.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 1.600 | 1318.928 | . 25% 1778070.000 | 698.000 | 637.000 | 104.000 | 238.000 | 77.000 | 35.000 | 0.000 | 15.000 | 238.000 | ... | 52213.750 | 5235.500 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 2.100 | 3773.952 | . 50% 4499692.000 | 5544.000 | 5214.000 | 394.000 | 1032.000 | 168.000 | 91.000 | 323.000 | 163.000 | 1032.000 | ... | 154207.000 | 14539.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 2.500 | 11557.920 | . 75% 7797095.000 | 22753.000 | 20815.500 | 938.000 | 3445.750 | 453.250 | 238.000 | 3413.000 | 840.000 | 3445.750 | ... | 399926.000 | 37408.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 3.100 | 19124.737 | . max 39937489.000 | 395872.000 | 553611.000 | 18825.000 | 89995.000 | 5225.000 | 2425.000 | 93572.000 | 24885.000 | 89995.000 | ... | 2279841.000 | 395872.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 4.800 | 71887.480 | . 8 rows × 22 columns . # drop unnecessary columns covid_cleaned = covid_df.drop([&#39;hospitalized&#39;, &#39;bedsPerThousand&#39;], axis=1) covid_100k = covid_cleaned.copy() # list of columns to transform to per 100k columns_list = [&#39;positive&#39;, &#39;active&#39;, &#39;recovered&#39;, &#39;death&#39;, &#39;hospitalizedCurrently&#39;, &#39;hospitalizedCumulative&#39;, &#39;inIcuCurrently&#39;, &#39;onVentilatorCurrently&#39;, &#39;total_beds&#39;] # add columns per 100k for column in columns_list: if column == &#39;total_beds&#39;: covid_100k[&#39;BedsPer100k&#39;.format(column)] = (covid_cleaned.loc[:, column] / covid_cleaned.loc[:, &#39;population&#39;]) * 100000 else: covid_100k[&#39;{}_100k&#39;.format(column)] = (covid_cleaned.loc[:, column] / covid_cleaned.loc[:, &#39;population&#39;]) * 100000 covid_100k = covid_100k.drop(columns_list, axis=1) . covid_100k[&#39;date&#39;] = pd.to_datetime(covid_100k[&#39;date&#39;]) start_date = &#39;2020-04-18&#39; end_date = &#39;2020-05-19&#39; mask = (covid_100k[&#39;date&#39;] &gt; start_date) &amp; (covid_100k[&#39;date&#39;] &lt;= end_date) covid_100k_last_month = covid_100k.loc[mask] . covid_100k_last_month_part1 = covid_100k_last_month.groupby(&#39;date&#39;).sum().loc[:, [&#39;positive_100k&#39;,&#39;active_100k&#39;,&#39;recovered_100k&#39;,&#39;death_100k&#39;,&#39;hospitalizedCumulative_100k&#39;]].diff(periods=1, axis=0) covid_100k_last_month_part2 = covid_100k_last_month.groupby(&#39;date&#39;).sum().loc[:, [&#39;inIcuCurrently_100k&#39;,&#39;onVentilatorCurrently_100k&#39;,&#39;BedsPer100k&#39;]] final_100k_last_month = covid_100k_last_month_part1.merge(covid_100k_last_month_part2, left_index=True, right_index=True) . final_100k_last_month.head() . positive_100k active_100k recovered_100k death_100k hospitalizedCumulative_100k inIcuCurrently_100k onVentilatorCurrently_100k BedsPer100k . date . 2020-04-19 nan | nan | nan | nan | nan | 152.818 | 80.717 | 13440.000 | . 2020-04-20 413.759 | 391.692 | 35.481 | 25.728 | 22.652 | 155.542 | 79.710 | 13440.000 | . 2020-04-21 387.394 | 360.446 | 65.218 | 30.520 | 31.446 | 164.605 | 78.603 | 13440.000 | . 2020-04-22 428.601 | 989.954 | 412.625 | 28.780 | 36.181 | 165.884 | 78.032 | 13440.000 | . 2020-04-23 452.031 | -2213.482 | 72.921 | 26.282 | 28.842 | 164.122 | 94.521 | 13440.000 | . final_100k_last_month.describe() . positive_100k active_100k recovered_100k death_100k hospitalizedCumulative_100k inIcuCurrently_100k onVentilatorCurrently_100k BedsPer100k . count 30.000 | 30.000 | 30.000 | 30.000 | 30.000 | 31.000 | 31.000 | 31.000 | . mean 399.188 | 364.943 | 147.172 | 23.063 | 39.160 | 139.595 | 73.503 | 13440.000 | . std 58.939 | 634.169 | 81.341 | 6.102 | 43.524 | 17.123 | 8.141 | 0.000 | . min 287.019 | -2213.482 | 35.481 | 13.053 | 9.507 | 109.602 | 61.622 | 13440.000 | . 25% 348.980 | 314.204 | 80.563 | 17.951 | 22.991 | 126.370 | 66.261 | 13440.000 | . 50% 405.026 | 366.234 | 127.774 | 24.119 | 28.295 | 140.327 | 74.706 | 13440.000 | . 75% 432.647 | 419.664 | 212.491 | 26.243 | 32.754 | 151.795 | 79.157 | 13440.000 | . max 544.349 | 2291.210 | 412.625 | 33.917 | 246.371 | 165.884 | 94.521 | 13440.000 | . # save description cleaned dataset to csv describe_file = &#39;results/final_100k_last_month.csv&#39; final_100k_last_month.describe().to_csv(describe_file) . Graphical Exploratory Analysis . Plotting histograms, scatterplots and boxplots to assess the distribution of the entire US dataset. . # Omitting the categorical (states/abbreviations) and time columns # There must be an easier way for you, but this was the easiest way I could think of covid_cleaned[&#39;date&#39;] = pd.to_datetime(covid_cleaned[&#39;date&#39;]) # mask data for last month start_date = &#39;2020-04-18&#39; end_date = &#39;2020-05-19&#39; mask = (covid_cleaned[&#39;date&#39;] &gt; start_date) &amp; (covid_cleaned[&#39;date&#39;] &lt;= end_date) covid_cleaned_last_month = covid_cleaned.loc[mask] plot_df = covid_cleaned_last_month[[&#39;population&#39;, &#39;active&#39;, &#39;recovered&#39;, &#39;death&#39;, &#39;hospitalizedCurrently&#39;, &#39;inIcuCurrently&#39;, &#39;onVentilatorCurrently&#39;, &#39;total_beds&#39;]] plot_df_last_month = covid_100k_last_month[[&#39;population&#39;, &#39;active_100k&#39;, &#39;recovered_100k&#39;, &#39;death_100k&#39;, &#39;hospitalizedCurrently_100k&#39;, &#39;inIcuCurrently_100k&#39;, &#39;onVentilatorCurrently_100k&#39;, &#39;BedsPer100k&#39;]] . timeseries_usa_df = covid_100k.loc[:, [&#39;date&#39;, &#39;positive_100k&#39;, &#39;active_100k&#39;, &#39;recovered_100k&#39;, &#39;death_100k&#39;, &#39;hospitalizedCurrently_100k&#39;, &#39;inIcuCurrently_100k&#39;, &#39;onVentilatorCurrently_100k&#39;, &#39;BedsPer100k&#39;]].groupby(&#39;date&#39;).sum().reset_index() # timeseries_usa_df[&#39;log_positive&#39;] = np.log(timeseries_usa_df[&#39;positive_100k&#39;]) # timeseries_usa_df[&#39;log_active&#39;] = np.log(timeseries_usa_df[&#39;active_100k&#39;]) # timeseries_usa_df[&#39;log_recovered&#39;] = np.log(timeseries_usa_df[&#39;recovered_100k&#39;]) # timeseries_usa_df[&#39;log_death&#39;] = np.log(timeseries_usa_df[&#39;death_100k&#39;]) . timeseries_usa_df.tail() . date positive_100k active_100k recovered_100k death_100k hospitalizedCurrently_100k inIcuCurrently_100k onVentilatorCurrently_100k BedsPer100k . 159 2020-06-29 | 35834.140 | 34810.986 | 13269.151 | 1605.037 | 416.665 | 67.216 | 32.901 | 13440.000 | . 160 2020-06-30 | 36369.224 | 33327.208 | 13616.885 | 1612.910 | 430.910 | 66.869 | 33.482 | 13440.000 | . 161 2020-07-01 | 36972.209 | 34868.331 | 13814.722 | 1622.259 | 443.622 | 67.228 | 34.721 | 13440.000 | . 162 2020-07-02 | 37608.249 | 21593.275 | 14384.117 | 1630.857 | 452.273 | 69.727 | 35.282 | 13440.000 | . 163 2020-07-03 | 38289.967 | 22177.125 | 14473.039 | 1639.803 | 451.256 | 71.286 | 34.451 | 13440.000 | . # get data from last day # plot_df_last_date = plot_df.loc[covid_df[&#39;date&#39;] == &#39;2020-05-18&#39;] # Plotting histograms to gain insight of the distribution shape, skewness and scale fig, axs = plt.subplots(4,2,figsize = (16, 16)) sns.set() for i, column in enumerate(plot_df_last_month.columns): if (i + 1) % 2 == 0: ax = axs[(i//2), 1] else: ax = axs[(i//2), 0] sns.distplot(plot_df_last_month[column], fit=norm, fit_kws=dict(label=&#39;normality&#39;), hist_kws=dict(color=&#39;plum&#39;, edgecolor=&#39;k&#39;, linewidth=1, label=&#39;frequency&#39;), ax=ax, color=&#39;#9d53ad&#39;) ax.legend(loc=&#39;upper right&#39;) plt.tight_layout() fig.subplots_adjust(top=0.95) . # Looking at linearity and variance with scatterplots # Removing the target variable and saving it in another df target = plot_df.hospitalizedCurrently indep_var = plot_df.drop(columns=[&#39;hospitalizedCurrently&#39;]) fig, ax = plt.subplots(figsize = (16, 16)) for i, col in enumerate(indep_var.columns): ax=fig.add_subplot(4, 3, i+1) sns.regplot(x=indep_var[col], y=target, data=indep_var, label=col, scatter_kws={&#39;s&#39;:10}, line_kws={&quot;color&quot;: &quot;plum&quot;, &#39;label&#39;: &#39;hospitCurr&#39;}) plt.suptitle(&#39;Scatterplots with Target Hospitalized Patients Showing Growth Trajectories&#39;, fontsize=23) plt.legend() plt.tight_layout() fig.subplots_adjust(top=0.95) . # Assessing the normality of the distribution with a boxplot # Boxplot with removed outliers fig, ax = plt.subplots(figsize = (16, 12)) for i, col in enumerate(plot_df.columns): ax=fig.add_subplot(4, 3, i+1) sns.boxplot(x=plot_df[col], data=plot_df, color=&#39;lightblue&#39;, showfliers=False) plt.suptitle(&#39;Boxplots of Independent Variables&#39;, fontsize=23) plt.tight_layout() fig.subplots_adjust(top=0.95) . # get data from last day plot_df_last_date = plot_df.loc[covid_df[&#39;date&#39;] == &#39;2020-05-18&#39;] fig, ax = plt.subplots(figsize = (16, 12)) for i, col in enumerate(plot_df_last_date.columns): ax=fig.add_subplot(4, 3, i+1) sns.boxplot(x=plot_df_last_date[col], data=plot_df, color=&#39;lightblue&#39;, showfliers=True) plt.suptitle(&#39;Boxplots of Independent Variables&#39;, fontsize=23) plt.tight_layout() fig.subplots_adjust(top=0.95) . Analysis of Hospitalizations by State . Since the normality of the independent variables is highly variable do to temporal and precision differences from each state, we further assess each state&#39;s data by viewing trends on the independent variables in both scatter and box-and-whisker plots. . Alabama . C: Users Doctor Gomez AppData Roaming Python Python37 site-packages pandas plotting _converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters. To register the converters: &gt;&gt;&gt; from pandas.plotting import register_matplotlib_converters &gt;&gt;&gt; register_matplotlib_converters() . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Arizona . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;% Positive Cases in Hospital&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Arkansas . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . California . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Colorado . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Connecticut . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Delaware . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Florida . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . # TODO fix legend/axis/plot alltogether # Timeseries plt fig, ax = plt.subplots(figsize = (16, 12)) plt.plot(fl.date, fl.positiveTestsViral, linewidth=4.7, color=&#39;r&#39;) plt.title(&#39;Cummulative Number of Positive Viral Tests in Florida&#39;, fontsize=23) plt.xlabel(&#39;Date&#39;) plt.ylabel(&#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;% Infected&#39;) . Georgia . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;% Infection Rate&#39;) . Hawaii . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Text(0, 0.5, &#39;% Infected&#39;) . Idaho . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Iowa . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Kansas . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Kentucky . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Louisiana . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Maine . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Maryland . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Massachusetts . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Michigan . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Minnesota . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Mississippi . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Missouri . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Montana . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Nebraska . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Nevada: . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . New Hampshire . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . New Jersey . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . New Mexico . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . New York . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . North Carolina . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Ohio . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Oklahoma . Text(0, 0.5, &#39;No. Patients&#39;) . Oregon . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Pennsylvania . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Rhode Island . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . South Carolina . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;% Infection Rate&#39;) . South Dakota . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Tennessee . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Texas . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Utah . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Vermont . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Virginia . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Washington . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . West Virginia . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Wisconsin . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Wyoming . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Patients&#39;) . Text(0, 0.5, &#39;No. Killed&#39;) . Assessing Correlation of Independent Variables . # TODO add some explanation / look more into collinear variables . # Heatmap of correlations # Save correlations to variable corr = covid_cleaned.corr(method=&#39;pearson&#39;) # We can create a mask to not show duplicate values mask = np.triu(np.ones_like(corr, dtype=np.bool)) # Set up the matplotlib figure fig, ax = plt.subplots(figsize=(16,16)) # Generate heatmap sns.heatmap(corr, annot=True, mask=mask, cmap=&#39;GnBu&#39;, center=0, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x25010c20748&gt; . Build model for dependent Variable . To be used to predict current hospitalizations | Having more complete variables for in ICU currently and on Ventilator Currently will allow us to predict these numbers as well. | . # We compare three models: # - Polynomial Regression # - Linear Regression # - ElasticNet # Copy DFs to not mess up original one # We will use model_df for our regression model model_df = all_cases.copy() # Delete redundant rows for row in [&#39;abbrev&#39;, &#39;bedsPerThousand&#39;, &#39;hospitalized&#39;, &#39;state&#39;, &#39;hospitalizedCumulative&#39;, &#39;dataQualityGrade&#39;, &#39;lastUpdateEt&#39;]: del model_df[row] # Drop NaN values for hospitalizedCurrently model_df = model_df.dropna(subset=[&#39;hospitalizedCurrently&#39;]) # Drop Values with abnormal active-hospitalised ratios (outside Conf. Interval) model_df[&#39;ratio_hospital&#39;] = model_df[&#39;hospitalizedCurrently&#39;] / model_df[&#39;active&#39;] model_df = model_df[~(model_df[&#39;ratio_hospital&#39;] &gt;= model_df.ratio_hospital.quantile(0.99))] #model_df = model_df[~(model_df[&#39;ratio_hospital&#39;] &lt;= model_df[&#39;ratio_hospital&#39;].median())] del model_df[&#39;ratio_hospital&#39;] # Get peek of model to use model_df.describe() . population positive active hospitalizedCurrently inIcuCurrently onVentilatorCurrently recovered death totalTestsViral positiveTestsViral negativeTestsViral positiveCasesViral commercialScore negativeRegularScore negativeScore positiveScore score grade total_beds . count 4016.000 | 4016.000 | 4016.000 | 4016.000 | 2003.000 | 1742.000 | 4016.000 | 4016.000 | 1330.000 | 443.000 | 448.000 | 2865.000 | 4016.000 | 4016.000 | 4016.000 | 4016.000 | 4016.000 | 0.000 | 4016.000 | . mean 6599959.463 | 32056.054 | 29434.070 | 972.991 | 431.098 | 213.277 | 7421.054 | 1691.020 | 407045.865 | 26715.413 | 266489.357 | 36657.264 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 15743.010 | . std 7580186.713 | 56930.101 | 52375.376 | 1867.806 | 705.513 | 322.794 | 13907.212 | 3589.041 | 578843.972 | 26788.099 | 249374.376 | 61167.272 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 16214.148 | . min 567025.000 | 115.000 | 113.000 | 1.000 | 1.000 | 0.000 | 0.000 | 0.000 | 2857.000 | 407.000 | 8648.000 | 396.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 1318.928 | . 25% 1778070.000 | 3253.000 | 3050.000 | 103.000 | 75.000 | 32.000 | 35.000 | 89.000 | 72778.750 | 4150.000 | 66084.000 | 6277.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 3773.952 | . 50% 4499692.000 | 12498.500 | 11350.000 | 390.500 | 165.000 | 90.000 | 1513.000 | 459.000 | 213753.000 | 14498.000 | 186700.000 | 16661.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 11557.920 | . 75% 7797095.000 | 35407.250 | 32086.000 | 927.500 | 430.000 | 228.000 | 6783.750 | 1506.500 | 499122.500 | 46468.000 | 380138.250 | 40786.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 19124.737 | . max 39937489.000 | 395872.000 | 553611.000 | 18825.000 | 5225.000 | 2425.000 | 93572.000 | 24885.000 | 4448176.000 | 89648.000 | 1036090.000 | 395872.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | nan | 71887.480 | .",
            "url": "https://bielsnor.github.io/fp/futureproof-COVID19-us-hosp/",
            "relUrl": "/futureproof-COVID19-us-hosp/",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ . import altair as alt from vega_datasets import data counties = alt.topo_feature(data.us_10m.url, &#39;counties&#39;) source = data.unemployment.url alt.Chart(counties).mark_geoshape().encode( color=&#39;rate:Q&#39; ).transform_lookup( lookup=&#39;id&#39;, from_=alt.LookupData(source, &#39;id&#39;, [&#39;rate&#39;]) ).project( type=&#39;albersUsa&#39; ).properties( width=500, height=300 ) .",
            "url": "https://bielsnor.github.io/fp/jupyter/2020/03/07/noob.html",
            "relUrl": "/jupyter/2020/03/07/noob.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bielsnor.github.io/fp/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bielsnor.github.io/fp/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bielsnor.github.io/fp/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bielsnor.github.io/fp/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}